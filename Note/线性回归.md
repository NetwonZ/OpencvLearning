# 线性回归与梯度下降

- ##### 线性回归

  在给定一个函数$Y^i=\Theta·X^i$时,我们想要去逼近这个函数,我们定义一个**损失函数**
  $$
  J(\theta) = \sum_{i=1}^{m}(Y^i - \hat{Y^i})^2
  $$
  上式中$ \Theta = \begin{bmatrix}\Theta_1&\Theta_2& \cdots &\Theta_n  \end{bmatrix}^T $ ,而上标$i$表示第$i$个点.

  而$X = \begin{bmatrix}x_1&x_2&\cdots&x_n\end{bmatrix}$ 表示$Y^i$有$n$个相关的因子

- ##### 梯度下降算法

  $\Theta$矩阵一个未知数的集合,需要我们找到合适的值,令$\hat{Y}$和$Y$的值最接近,即$J(\Theta)$最小

  由高中知识,求一个函数的极值点,我们可以对该函数进行求导,导函数的零点就是函数的极值点
  $$
  \nabla{J(\Theta)} = \begin{bmatrix} \frac{\partial J}{\partial\theta_1}\\ \frac{\partial J}{\partial \theta_2}\\ \vdots \\ \frac{\partial J}{\partial \theta_n} \end{bmatrix}
  $$
  举简单一例$\frac{\partial J}{\partial \theta_1} = \sum_i^m 2(Y^i-\theta_1x^i_1-\theta_2x_2^i-\cdots\theta_nx_n^i)\pmb{(-x_1^i)}$ 

  由上式类推
  $$
  \nabla{J(\Theta)} =2 \begin{bmatrix} \sum_{i=1}^m(Y^i - A)(-x^i_1)\\\sum_{i=1}^mY^i - A)(-x^i_2)\\ \vdots \\ \sum_{i=1}^m(Y^i - A)(-x^i_n) \end{bmatrix}
  $$
  有时$J(\Theta)$会很大,所以我们可以除以$m$(样本个数),也可以再除以2与求偏导的2抵消.得到如下
  $$
  J(\Theta) = \frac{1}{2m}MSE(Y,\hat{Y})
  $$
  
  - $\hat{Y}$的式子如下上标表示第几个点,下标表示第几个特征
    $$
    \begin{bmatrix}Y^1&Y^2&\cdots&Y^m \end{bmatrix} =\begin{bmatrix} \theta_1&\theta_2&\cdots&\theta_n\end{bmatrix} · \begin{bmatrix}x^1_1&x^2_1&\cdots&x^m_1\\x^1_2&x^2_2&\cdots&x^m_2\\ \vdots&\vdots&\ddots&\vdots\\x^1_n&x^2_n&\cdots&x^m_n \end{bmatrix}
    $$
  
  在迭代中更新的是$\Theta$$\Theta$更新的方式为$\Theta_i = \Theta_{i-1} - \nabla J$
  
  代码实现:
  
  ```python
  import cv2
  import matplotlib.pyplot as plt
  import numpy as np
  
  x_l = np.arange(0,10,0.2) #(50,)
  m = len(x_l)    
  x0 = np.full(m, 1.0)  
  Y = 4*x_l + 3 +np.random.randn(m)
  Y = Y.reshape(1,m)
  #Y^ = w0+w*x = Theta*X
  X = np.vstack([x0,x_l])#(2,50)
  plt.scatter(x_l,Y)
  plt.show()
  Theta = np.array([[13],[62]])
  
  
  def bgd(alpha,loops,epsilon):
      # [批量梯度下降]
      # alpha:步长, 
      # loops:循环次数,
      # epsilon:收敛精度
      global Theta
      count = 0
      while count < loops:
          count += 1
          Y_pred = Theta.T.dot(X)
          Loss = ((Y-Y_pred)**2).sum()/(2*m)
          nabla_L = ((Y - Y_pred).dot(-X.T).T)/m
          Theta = Theta - alpha*nabla_L
          
          if count%25 == 0 :
              print("The epoch:",count,"-Loss = ",Loss)
  
  
  
  
  if __name__ == '__main__':
      
      bgd(0.01,600,0.0001)
      Theta = Theta.ravel()
      print("The result,_W_0_:",Theta[0],"_W_1_:",Theta[1])
  ```
  
  设置目标函数$Y=4x+3$ 初始的默认值设置为$62,13$ 通过梯度下降逼近结果
  
  > #输出的结果接近,但是任然有一定的差距,Loss停留在0.5左右
  >
  > The epoch: 25 -Loss =  0.720760746588419
  > The epoch: 50 -Loss =  0.707490986974396
  > The epoch: 75 -Loss =  0.6960046068711356
  > The epoch: 100 -Loss =  0.6858783901242498
  > The epoch: 125 -Loss =  0.6769512725565638
  > The epoch: 150 -Loss =  0.6690812624571058
  > The epoch: 175 -Loss =  0.6621431821085535
  > The epoch: 200 -Loss =  0.6560266767528263
  > The epoch: 225 -Loss =  0.6506344593256458
  > The epoch: 250 -Loss =  0.645880763041417
  > The epoch: 275 -Loss =  0.6416899772157589
  > The epoch: 300 -Loss =  0.6379954446275495
  > The epoch: 325 -Loss =  0.6347384012917212
  > The epoch: 350 -Loss =  0.631867041779204
  > The epoch: 375 -Loss =  0.6293356952172973
  > The epoch: 400 -Loss =  0.6271040988642329
  > The epoch: 425 -Loss =  0.6251367577036212
  > The epoch: 450 -Loss =  0.6234023798727357
  > The epoch: 475 -Loss =  0.6218733789447373
  > The epoch: 500 -Loss =  0.6205254351483004
  > The epoch: 525 -Loss =  0.6193371085455616
  > The epoch: 550 -Loss =  0.6182894980157229
  > The epoch: 575 -Loss =  0.617365940620224
  > The epoch: 600 -Loss =  0.6165517465676751
  > The result,_W_0_: 3.3621622259618142 _W_1_: 4.015208451343468

