# 线性回归与梯度下降

- ##### 线性回归

  在给定一个函数$Y^i=\Theta·X^i$时,我们想要去逼近这个函数,我们定义一个**损失函数**
  
  $$
  J(\theta) = \sum_{i=1}^{m}(Y^i - \hat{Y^i})^2
  $$
  
  上式中 $ \Theta = \begin{bmatrix}\Theta_1&\Theta_2& \cdots &\Theta_n  \end{bmatrix}^T $  ,而上标$i$表示第$i$个点.

  而 $X = \begin{bmatrix}x_1&x_2&\cdots&x_n\end{bmatrix}$ 表示 $Y^i$有$n$个相关的因子

- ##### 梯度下降算法

  $\Theta$矩阵一个未知数的集合,需要我们找到合适的值,令$\hat{Y}$和$Y$的值最接近,即$J(\Theta)$最小

  由高中知识,求一个函数的极值点,我们可以对该函数进行求导,导函数的零点就是函数的极值点
  
  $$
  \nabla{J(\Theta)} = \begin{bmatrix} \frac{\partial J}{\partial\theta_1}\\ \frac{\partial J}{\partial \theta_2}\\ \vdots \\ \frac{\partial J}{\partial \theta_n} \end{bmatrix}
  $$
  
  举简单一例 $\frac{\partial J}{\partial \theta_1} = \sum_i^m 2(Y^i-\theta_1x^i_1-\theta_2x_2^i-\cdots\theta_nx_n^i)\pmb{(-x_1^i)}$ 

  由上式类推
  
  $$
  \nabla{J(\Theta)} =2 \begin{bmatrix} \sum_{i=1}^m(Y^i - A)(-x^i_1)\\\sum_{i=1}^mY^i - A)(-x^i_2)\\ \vdots \\ \sum_{i=1}^m(Y^i - A)(-x^i_n) \end{bmatrix}
  $$
  
  有时$J(\Theta)$会很大,所以我们可以除以$m$(样本个数),也可以再除以2与求偏导的2抵消.得到如下
  
  $$
  J(\Theta) = \frac{1}{2m}MSE(Y,\hat{Y})
  $$

- 

